---
layout: post
cover: assets/images/yoloventana.jpg
title: Deep Learning Datasets in the COVID era
date: 2020-05-28 12:00:00 +0545
categories: deep learning dataset database google
author: olaya
featured: true
summary: creating a dataset from internet images.
---

From 2020 I was expecting to collect a dataset for my Deep Learning PhD project,
but instead, 2020 brought me quarantine. This was quite frustrating because,
how can I keep going with my project, if I can't even make the first and more basic
step?

Well, I guess that once again, the answer is in the internet. Aren't there millions
of pictures on the internet that I can use to make my own database? Well, sure there are.
However, this idea arises some questions:

- Is there any tool that can help me with this? I could just google "cat pics"
(just to keep going with the typical cat example) and download the ones that fit my needs,
but that seems extremely time-consuming and ineffective.
- What about image rights? I'm not quite sure about the rights of using images
for a dataset, so I'll show you my findings on this in this post.

# Gathering Images for your database

Here, I would go through the most interesting methods I found to gather Images
from the internet (Google, Bing, and Flickr), to see which of them are suitable
for creating a Machine Learning database.

## From Google
Google is the most popular search engine, so no wonder that there are different
alternatives online to download pictures from it.

### Making custom scripts
The first solution I found for this was from the great blog
[pyimagesearch](https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/)
(seriously, is there any computer vision problem for which this blog doesn't
already have a solution?). I won't come into details since it is already clearly
explained in the original post, but I'll make a quick resume so you can get an idea
of how much human effort this solution requires. It can be basically divided in
three steps:

1. Search the query term on Google images.
2. Gather the URLs into a text file using the JavaScript console. You'll scroll
 through the results, and the script will simulate clicking on them and copying
 the URL for you.
3. Download the images from the URLs using a Python script.

This is indeed a very interesting solution. However, I see two inconvenients here:
First, you need to scroll through the results. It is not that much of a big deal,
but still would require your time on something that could be automated. And second,
although the google results are very good (and always improving) there are always
incorrect results  or duplicates on the results. Adrian proposes a manual search
for this cases, which will again consume your time and attention, but there is no
really a straightforward solution for this.

### Google Image downloaders

I found a bunch of python libraries like
[this](https://pypi.org/project/google_images_download/2.3.0/), or
[this one](https://pypi.org/project/googleimagedownloader/) that are supposed to
download pictures from Google Images with just a simple bash command or a shell
script. However, they recently stopped working.
The next workaround was a similar script,
[but this time for Bing](https://medium.com/@yfujiki/googleimagesdownload-is-dead-long-live-bingimagesdownload-fb9f5d3b4296).
But just like the others, it no longer works.

They all launch the same error: the images are not downloadable. There are
several Github issues around this problem, and it seems to be that the links
created by the scripts no longer work because Google has recently changed the
way they present the data (and I guess, same for Bing).

I'm sorry for the bad news, but I think this should be noted out to avoid you
from losing time by trying to make these libraries work. But don't worry,
hereafter I'm showing you the solutions that work!

### Simple Image Download
[This script](https://github.com/RiddlerQ/simple_image_download)
is pretty simple but is the only one that I found that works with Google up to date.
You basically define the Search query and the maximum number of pictures,
and it downloads them for you.

I tried it by downloading 200 pictures, and I'd say that up to this point
this is the best solution, since it is very straightforward and
it worked perfectly fine.


## Flickr Scraper
There's this similar alternative to the Google Images scripts, but
[with Flickr](https://github.com/ultralytics/flickr_scraper).
And yes, this one works!

You need a Flickr account in order to use it, and ask for permissions
(non-commercial in my case). They also ask you for data about your "app", and to
agree the API terms of use. Nobody likes making more accounts and accepting
more terms, and I agree with you in it being a headache, but I have to say that
in this case it's very convenient, because it will help us with the whole
"rights of use" thing.
I've read the terms (yes, I know), and by default the photos are owned
by the user that uploaded them. If the photo is marked as "All Rights Reserved",
the photo cannot be copied or used in any way without permission. So if you
want to use it, you must ask the owner for permission to do it.
The Flickr API has its own terms, but it clearly says that it use must
"comply with any other terms and conditions a user has attached to his or her
photo".
Unfortunately the Flickr Scraper doesn't filter which rights the image has,
however, since it is in the metadata of the image, maybe there is a way to
implement that.

So, to resume:

  :heavy_check_mark: FLickr Scraper works pretty easily and well.

  :x: however, it requires an user account

  :x: and it doesn't take into account the right permissions of the pictures.


# Rights of use for ML Datasets

I've mentioned so far the (un)existence of filters for selecting the images to
download according to their rights of use... but which rights of use do we need
to use a picture in our dataset?

I want to emphasize that this debate doesn't apply to the training
of the algorithm: Copyright applies to **copying** data, so as long as the original images
cannot be reconstructed from the algorithm, we're covered. The problem arises when making
this dataset publicly available.

Some datasets based in public images are Google's Open Images, Microsoft's COCO,
ImageNet, etc. They save the URLs of the images that form the dataset,
but this leads to an important data loss if the URL breaks due to Copyright or
banning issues, to name a few.

If you save a cached copy of the images, there is no problem if you use them
internally, but in case of wanting to offer them as a public download, you must
take the Copyright into account. You would have to use public domain data or
data with a Creative Commons license that is permissive enough. Here is a table
that shows the different kinds of CC licenses:
![alt text](https://github.com/olayasturias/olayasturias.github.io/tree/master/assets/images/CClicense.jpg "CC licenses")

The most restrictive CC license doesn't allow to use the picture for commercial
purposes and/or modifying it, and you need to credit the author, so, let's put this
into (my) situation:

:heavy_check_mark: I'm a researcher, so no commercial use is being made.
:heavy_check_mark: I'm downloading the picture as is, so no modification is being
made. However, :x: no data augmentation could be done with such pictures!
:heavy_check_mark: If I keep the metadata of the picture, I'm keeping the attribution.


Moreover, different countries offer different regulations for data mining and
research purposes. In the European Union we have the *Copyright Final Compromise*
in the Article 13, but it is not very clear and it seems to talk more about the
need of regulation on this, rather than on the regulation itself. Here some of the
more interesting (yet confusing) paragraphs that I read:
> Mandatory exceptions or limitations for uses of text and data mining technologies,
illustration for teaching in the digital environment and for preservation of
cultural heritage should be introduced.
> The exceptions and limitations existing in Union law should continue to apply,
including to text and data mining, education and preservation activities,
as long as they do not limit the scope of the mandatory exceptions laid down
in this Directive, which need to be implemented by Member States in their national law.
>
> [..]
> Text and data mining may also be carried out in relation to mere facts or data
which are not protected by copyright and in such instances no authorisation is
required under copyright law. There may also beinstances of text and data mining
which do not involve acts of reproduction or where the reproductions made fall
under the mandatory exception for temporary actsof reproduction laid down in
Article 5(1) of Directive 2001/29/EC, which should continue to apply to text and
data mining techniques which do not involve the making of copies beyond the scope
of that exception.
> [..]
> (11b)Researchorganisations and cultural heritage institutions, including the persons attached thereto, should be covered by the text and data mining exception regarding content to which they have lawful access. Lawful access should be understood as covering access to content based on open access policy or through contractual arrangements between rightholders and research organisations or cultural heritage institutions, such as subscriptions, or through other lawful means. For instance, in cases of subscriptions taken by research organisations or cultural heritage institutions, the persons attached thereto covered by these subscriptions would be deemed to have lawful access. Lawful access also covers access to content that is freely available online

This is an unfinished article, as this subject is not very clear for me and I'll
have to keep investigating.
